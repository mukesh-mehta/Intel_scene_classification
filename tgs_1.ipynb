{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Augmentation\n",
    "##### Horizontal Flip\n",
    "##### Zoom\n",
    "##### shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data'\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TernausNet.unet_models import UNetResNet34,UNet11_bn_dilated,UNet11_bn,UNet11_bn_v2\n",
    "\n",
    "def unet_model(**kwargs):\n",
    "    model = UNet11_bn_dilated(**kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    model = unet_model()\n",
    "    model.train()\n",
    "    return model.cuda()\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, dice_weight=1):\n",
    "        self.nll_loss = nn.BCELoss()\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        loss = self.nll_loss(outputs, targets)\n",
    "        if self.dice_weight:\n",
    "            eps = 1e-15\n",
    "            dice_target = (targets == 1).float()\n",
    "            dice_output = outputs\n",
    "            intersection = (dice_output * dice_target).sum()\n",
    "            union = dice_output.sum() + dice_target.sum() + eps\n",
    "\n",
    "            dice_loss = 1 - (2 * intersection / union)\n",
    "\n",
    "        return loss - torch.log(dice_loss)\n",
    "    \n",
    "class RobustFocalLoss2d(nn.Module):\n",
    "    #assume top 10% is outliers\n",
    "    def __init__(self, gamma=2, size_average=True):\n",
    "        super(RobustFocalLoss2d, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "\n",
    "\n",
    "    def forward(self, logit, target, class_weight=None, type='softmax'):\n",
    "        target = target.view(-1, 1).long()\n",
    "\n",
    "\n",
    "        if type=='sigmoid':\n",
    "            if class_weight is None:\n",
    "                class_weight = [1]*2 #[0.5, 0.5]\n",
    "\n",
    "#             prob   = F.sigmoid(logit)\n",
    "            prob=logit\n",
    "            prob   = prob.view(-1, 1)\n",
    "            prob   = torch.cat((1-prob, prob), 1)\n",
    "            select = torch.FloatTensor(len(prob), 2).zero_().cuda()\n",
    "            select.scatter_(1, target, 1.)\n",
    "\n",
    "        elif  type=='softmax':\n",
    "            B,C,H,W = logit.size()\n",
    "            if class_weight is None:\n",
    "                class_weight =[1]*C #[1/C]*C\n",
    "\n",
    "            logit   = logit.permute(0, 2, 3, 1).contiguous().view(-1, C)\n",
    "            prob    = F.softmax(logit,1)\n",
    "            select  = torch.FloatTensor(len(prob), C).zero_().cuda()\n",
    "            select.scatter_(1, target, 1.)\n",
    "\n",
    "        class_weight = torch.FloatTensor(class_weight).cuda().view(-1,1)\n",
    "        class_weight = torch.gather(class_weight, 0, target)\n",
    "\n",
    "        prob  = (prob*select).sum(1).view(-1,1)\n",
    "        prob  = torch.clamp(prob,1e-8,1-1e-8)\n",
    "\n",
    "        focus = torch.pow((1-prob), self.gamma)\n",
    "        #focus = torch.where(focus < 2.0, focus, torch.zeros(prob.size()).cuda())\n",
    "        focus = torch.clamp(focus,0,2)\n",
    "\n",
    "\n",
    "        batch_loss = - class_weight *focus*prob.log()\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def accuracy(prob, truth, threshold=0.5,  is_average=True):\n",
    "    batch_size = prob.size(0)\n",
    "    p = prob.detach().view(batch_size,-1)\n",
    "    t = truth.detach().view(batch_size,-1)\n",
    "\n",
    "    p = p>threshold\n",
    "    t = t>0.5\n",
    "    correct = ( p == t).float()\n",
    "    accuracy = correct.sum(1)/p.size(1)\n",
    "\n",
    "    if is_average:\n",
    "        accuracy = accuracy.sum()/batch_size\n",
    "        return accuracy\n",
    "    else:\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def random_crop_resize(img, scale=0.75):\n",
    "    center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n",
    "    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n",
    "    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "    img_cropped = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n",
    "    return cv2.resize(img_cropped,(101,101),interpolation = cv2.INTER_AREA)\n",
    "\n",
    "def increase_brightness(img, value=30):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, mask = False, h_flip=False, crop=False, brightness = False,ratio = 0.75):\n",
    "    \"\"\"\n",
    "    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)\n",
    "    \n",
    "    if pad = True:\n",
    "        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n",
    "    else:\n",
    "        returns image as numpy.array\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if h_flip:\n",
    "        img = cv2.flip( img, 1 )\n",
    "    if crop:\n",
    "        img = random_crop_resize(img,scale=ratio)\n",
    "    \n",
    "    if brightness:\n",
    "        img = increase_brightness(img)\n",
    "        \n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # Padding in needed for UNet models because they need image size to be divisible by 32 \n",
    "    if height % 32 == 0:\n",
    "        y_min_pad = 0\n",
    "        y_max_pad = 0\n",
    "    else:\n",
    "        y_pad = 32 - height % 32\n",
    "        y_min_pad = int(y_pad / 2)\n",
    "        y_max_pad = y_pad - y_min_pad\n",
    "        \n",
    "    if width % 32 == 0:\n",
    "        x_min_pad = 0\n",
    "        x_max_pad = 0\n",
    "    else:\n",
    "        x_pad = 32 - width % 32\n",
    "        x_min_pad = int(x_pad / 2)\n",
    "        x_max_pad = x_pad - x_min_pad\n",
    "    \n",
    "    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)\n",
    "    if mask:\n",
    "        # Convert mask to 0 and 1 format\n",
    "        img = img[:, :, 0:1] // 255\n",
    "        return torch.from_numpy(img).float().permute([2, 0, 1])\n",
    "    else:\n",
    "        img = img / 255.0\n",
    "        return torch.from_numpy(img).float().permute([2, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "img = cv2.imread('../data/train/masks/0a1742c740.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(gray)\n",
    "plt.title('my picture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('../data/train/images/0ba541766e.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(gray)\n",
    "plt.title('my picture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('../data/train/masks/0a1742c740.png')\n",
    "img = cv2.flip( img, 1 )\n",
    "height, width, _ = img.shape\n",
    "\n",
    "# Padding in needed for UNet models because they need image size to be divisible by 32 \n",
    "if height % 32 == 0:\n",
    "    y_min_pad = 0\n",
    "    y_max_pad = 0\n",
    "else:\n",
    "    y_pad = 32 - height % 32\n",
    "    y_min_pad = int(y_pad / 2)\n",
    "    y_max_pad = y_pad - y_min_pad\n",
    "\n",
    "if width % 32 == 0:\n",
    "    x_min_pad = 0\n",
    "    x_max_pad = 0\n",
    "else:\n",
    "    x_pad = 32 - width % 32\n",
    "    x_min_pad = int(x_pad / 2)\n",
    "    x_max_pad = x_pad - x_min_pad\n",
    "\n",
    "img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REFLECT_101)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(gray)\n",
    "plt.title('my picture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms.functional as Func\n",
    "from torch.utils import data\n",
    "import tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TGSSaltDataset(data.Dataset):\n",
    "    def __init__(self, root_path, file_list, is_test = False, h_flip=False,crop=False,brightness=False,ratio=0.75):\n",
    "        self.is_test = is_test\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "        self.h_flip = h_flip\n",
    "        self.crop=crop\n",
    "        self.crop_ratio = ratio\n",
    "        self.brightness = brightness\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, len(self.file_list)):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "        \n",
    "        file_id = self.file_list[index]\n",
    "        \n",
    "        image_folder = os.path.join(self.root_path, \"images\")\n",
    "        image_path = os.path.join(image_folder, file_id + \".png\")\n",
    "        \n",
    "        mask_folder = os.path.join(self.root_path, \"masks\")\n",
    "        mask_path = os.path.join(mask_folder, file_id + \".png\")\n",
    "        \n",
    "        image = load_image(image_path, h_flip=self.h_flip,crop = self.crop,ratio = self.crop_ratio,brightness=self.brightness)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return (image,)\n",
    "        else:\n",
    "            mask = load_image(mask_path, mask = True, h_flip=self.h_flip, crop=self.crop,ratio = self.crop_ratio,brightness=self.brightness)\n",
    "            return image,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depths_df = pd.read_csv(os.path.join(directory, 'train.csv'))\n",
    "train_df = pd.read_csv(\"../data/train.csv\", index_col=\"id\", usecols=[0])\n",
    "train_df[\"masks\"] = [cv2.imread(\"../data/train/masks/{}.png\".format(idx),0) / 255 for idx in train_df.index]\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(101, 2)\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "train_path = os.path.join(directory, 'train')\n",
    "file_list = list(train_df.index.values)\n",
    "file_list_train,file_list_val  = train_test_split(file_list,test_size=0.2,stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25600"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA_LAUNCH_BLOCKING=1\n",
    "# def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "#     state = {'state_dict': model.state_dict(),\n",
    "#              'optimizer' : optimizer.state_dict()}\n",
    "#     torch.save(state, checkpoint_path)\n",
    "#     print('model saved to %s' % checkpoint_path)\n",
    "def save_checkpoint(state, is_best):\n",
    "    filename='sgd_model/best_loss.pth.tar'\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation loss did not improve\")\n",
    "\n",
    "def save_checkpoint_acc(state, is_best):\n",
    "    filename='sgd_model/best_acc.pth.tar'\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")\n",
    "        \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)\n",
    "    \n",
    "\n",
    "# file_list_train,file_list_dev = train_test_split(file_train,test_size=0.1,random_state=2)\n",
    "dataset = torch.utils.data.ConcatDataset([\n",
    "     TGSSaltDataset(train_path, file_list_train,h_flip=True),\n",
    "     TGSSaltDataset(train_path, file_list_train),\n",
    "    TGSSaltDataset(train_path, file_list_train,h_flip=True,crop=True),\n",
    "     TGSSaltDataset(train_path, file_list_train,crop=True),\n",
    "    TGSSaltDataset(train_path, file_list_train,h_flip=True,brightness= True),\n",
    "     TGSSaltDataset(train_path, file_list_train,brightness= True),\n",
    "    TGSSaltDataset(train_path, file_list_train,h_flip=True,crop=True,brightness= True),\n",
    "     TGSSaltDataset(train_path, file_list_train,crop=True,brightness= True)\n",
    "])\n",
    "\n",
    "dataset_val = TGSSaltDataset(train_path, file_list_val)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'sgd_model/unetdil_0.042_loss.pth.tar' (trained for 18 epochs)\n",
      "learning_rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "100%|| 2134/2134 [45:34<00:00,  1.28s/it]\n",
      "  0%|          | 0/2134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.029,  Val loss: 0.044\n",
      "Train acc: 0.956, Val acc: 0.939\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2134/2134 [28:34<00:00,  1.24it/s]\n",
      "  0%|          | 0/2134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.028,  Val loss: 0.045\n",
      "Train acc: 0.958, Val acc: 0.939\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2134/2134 [23:37<00:00,  1.51it/s]\n",
      "  0%|          | 0/2134 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.028,  Val loss: 0.045\n",
      "Train acc: 0.959, Val acc: 0.940\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|     | 953/2134 [10:38<13:10,  1.49it/s]Process Process-23:\n",
      "Process Process-21:\n",
      "Process Process-24:\n",
      "Traceback (most recent call last):\n",
      "Process Process-22:\n",
      "Process Process-19:\n",
      "Process Process-20:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 106, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataset.py\", line 81, in __getitem__\n",
      "    return self.datasets[dataset_idx][sample_idx]\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"<ipython-input-9-6785d21cdfbc>\", line 42, in __getitem__\n",
      "    mask = load_image(mask_path, mask = True, h_flip=self.h_flip, crop=self.crop,ratio = self.crop_ratio,brightness=self.brightness)\n",
      "KeyboardInterrupt\n",
      "  File \"<ipython-input-5-15ea78c9bbda>\", line 10, in load_image\n",
      "    img = cv2.imread(str(path))\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fb21db85978>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 397, in __del__\n",
      "    def __del__(self):\n",
      "  File \"/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 227, in handler\n",
      "    _error_if_any_worker_fails()\n",
      "RuntimeError: DataLoader worker (pid 7272) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-70a94cebe441>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m#         loss = loss_fn(y_pred, mask.to(device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdice\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# print(model)\n",
    "#\n",
    "def cyclic_lr(epoch, init_lr=1e-2, num_epochs_per_cycle=300, cycle_epochs_decay=75, lr_decay_factor=0.1):\n",
    "    epoch_in_cycle = epoch % num_epochs_per_cycle\n",
    "    lr = init_lr * (lr_decay_factor ** (epoch_in_cycle // cycle_epochs_decay))\n",
    "    return lr\n",
    "learning_rate = 1e-3\n",
    "# loss_fn = torch.nn.BCELoss()\n",
    "def criterion(logit, truth ):\n",
    "    loss = RobustFocalLoss2d()(logit, truth, type='sigmoid')\n",
    "    return loss\n",
    "\n",
    "def metric(logit, truth, threshold=0.5 ):\n",
    "#     prob = F.sigmoid(logit)\n",
    "    prob = logit\n",
    "    dice = accuracy(prob, truth, threshold=threshold, is_average=True)\n",
    "    return dice\n",
    "# loss_fn = FocalLoss2d()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.01)\n",
    "\n",
    "\n",
    "############\n",
    "# resume_weights = \"sgd_model/unetdil_0.042_loss.pth.tar\"\n",
    "# # if cuda:\n",
    "# checkpoint = torch.load(resume_weights)\n",
    "# start_epoch = checkpoint['epoch']\n",
    "# best_accuracy = checkpoint['best_accuracy']\n",
    "# model.load_state_dict(checkpoint['state_dict'])\n",
    "# print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))\n",
    "########################\n",
    "epoch=30\n",
    "best_loss=0.042\n",
    "best_acc = 0.945\n",
    "for e in range(epoch):\n",
    "#     learning_rate = cyclic_lr(e)\n",
    "    print(\"learning_rate\",learning_rate)\n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,momentum=0.9)\n",
    "#     scheduler.step()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    for image, mask in tqdm.tqdm(data.DataLoader(dataset, batch_size = 12,shuffle=True, num_workers=6)):\n",
    "        image = image.type(torch.FloatTensor).cuda()\n",
    "        y_pred = model(Variable(image))\n",
    "#         loss = loss_fn(y_pred, mask.to(device))\n",
    "        loss = criterion(y_pred, Variable(mask.cuda()))\n",
    "        dice  = metric(y_pred, Variable(mask.cuda()))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(dice.item())\n",
    "        \n",
    "    val_loss = []\n",
    "    val_acc =[]\n",
    "    for image, mask in data.DataLoader(dataset_val, batch_size = 8, shuffle = False):\n",
    "        image = image.cuda()\n",
    "        y_pred = model(Variable(image))\n",
    "\n",
    "        loss = criterion(y_pred, Variable(mask.cuda()))\n",
    "        dice  = metric(y_pred, Variable(mask.cuda()))\n",
    "        val_loss.append(loss.item())\n",
    "        val_acc.append(dice.item())\n",
    "    print(\"Epoch: %d, Train loss: %.3f,  Val loss: %.3f\" % (e, np.mean(train_loss), np.mean(val_loss)))\n",
    "    print(\"Train acc: %.3f, Val acc: %.3f\" %(np.mean(train_acc),np.mean(val_acc)))\n",
    "    val_loss = np.mean(val_loss)\n",
    "    is_best = bool(val_loss<best_loss)\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_loss = min(val_loss,best_loss)\n",
    "    # Save checkpoint if is a new best\n",
    "    save_checkpoint({\n",
    "        'epoch': e + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_accuracy': best_loss\n",
    "    }, is_best)\n",
    "    val_acc = np.mean(val_acc)\n",
    "    is_best_acc = bool(val_acc>best_acc)\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_acc = max(val_acc,best_acc)\n",
    "    # Save checkpoint if is a new best\n",
    "    save_checkpoint_acc({\n",
    "        'epoch': e + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_accuracy': best_acc\n",
    "    }, is_best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.94549072265625, 0.0423367481585592)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc,best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'save_model/best_acc.pth.tar' (trained for 166 epochs)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['fe14b9e031', '6c217077e5', 'dca0ca5ecd'], '../data/test')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "test_path = os.path.join(directory, 'test')\n",
    "test_file_list = glob.glob(os.path.join(test_path, 'images', '*.png'))\n",
    "test_file_list = [f.split('/')[-1].split('.')[0] for f in test_file_list]\n",
    "test_file_list[:3], test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/2250 [00:00<03:48,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2250/2250 [03:30<00:00, 10.68it/s]\n"
     ]
    }
   ],
   "source": [
    "#NOrmal Image\n",
    "print(len(test_file_list))\n",
    "test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n",
    "\n",
    "all_predictions = []\n",
    "for image in tqdm.tqdm(data.DataLoader(test_dataset, batch_size = 8)):\n",
    "    image = image[0].type(torch.FloatTensor).cuda()\n",
    "    y_pred = model(Variable(image)).cpu().data.numpy()\n",
    "    all_predictions.append(y_pred)\n",
    "all_predictions_stacked = np.vstack(all_predictions)[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2250 [00:00<03:13, 11.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2250/2250 [03:34<00:00, 10.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#Flipped Image\n",
    "print(len(test_file_list))\n",
    "test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True,h_flip=True)\n",
    "\n",
    "all_predictions_flip = []\n",
    "for image in tqdm.tqdm(data.DataLoader(test_dataset, batch_size = 8)):\n",
    "    image = image[0].type(torch.FloatTensor).cuda()\n",
    "    y_pred = model(Variable(image)).cpu().data.numpy()\n",
    "    all_predictions_flip.append(y_pred)\n",
    "all_predictions_stacked_flip = np.vstack(all_predictions_flip)[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 128, 128) (18000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions_stacked.shape,all_predictions_stacked_flip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "final_pred=[]\n",
    "for pred1, pred2 in zip(all_predictions_stacked,all_predictions_stacked_flip):\n",
    "    final_pred.append((pred1+np.fliplr(pred2))/2)\n",
    "final_pred = np.array(final_pred)\n",
    "print(final_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 101, 101\n",
    "\n",
    "if height % 32 == 0:\n",
    "    y_min_pad = 0\n",
    "    y_max_pad = 0\n",
    "else:\n",
    "    y_pad = 32 - height % 32\n",
    "    y_min_pad = int(y_pad / 2)\n",
    "    y_max_pad = y_pad - y_min_pad\n",
    "\n",
    "if width % 32 == 0:\n",
    "    x_min_pad = 0\n",
    "    x_max_pad = 0\n",
    "else:\n",
    "    x_pad = 32 - width % 32\n",
    "    x_min_pad = int(x_pad / 2)\n",
    "    x_max_pad = x_pad - x_min_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 101, 101)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred = final_pred[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n",
    "final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 200/200 [00:09<00:00, 21.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((800, 101, 101), (800, 101, 101))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n",
    "\n",
    "val_predictions = []\n",
    "val_masks = []\n",
    "for image, mask in tqdm.tqdm(data.DataLoader(dataset_val, batch_size = 4)):\n",
    "    image = Variable(image.type(torch.FloatTensor).cuda())\n",
    "    y_pred = model(image).cpu().data.numpy()\n",
    "    val_predictions.append(y_pred)\n",
    "    val_masks.append(mask)\n",
    "    \n",
    "val_predictions_stacked = np.vstack(val_predictions)[:, 0, :, :]\n",
    "\n",
    "val_masks_stacked = np.vstack(val_masks)[:, 0, :, :]\n",
    "val_predictions_stacked = val_predictions_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n",
    "\n",
    "val_masks_stacked = val_masks_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n",
    "val_masks_stacked.shape, val_predictions_stacked.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.0, Metric: 0.133\n",
      "Threshold: 0.1, Metric: 0.174\n",
      "Threshold: 0.2, Metric: 0.421\n",
      "Threshold: 0.3, Metric: 0.674\n",
      "Threshold: 0.4, Metric: 0.838\n",
      "Threshold: 0.5, Metric: 0.870\n",
      "Threshold: 0.6, Metric: 0.852\n",
      "Threshold: 0.7, Metric: 0.809\n",
      "Threshold: 0.8, Metric: 0.736\n",
      "Threshold: 0.9, Metric: 0.645\n",
      "Threshold: 1.0, Metric: 0.641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "metric_by_threshold = []\n",
    "for threshold in np.linspace(0, 1, 11):\n",
    "    val_binary_prediction = (val_predictions_stacked > threshold).astype(int)\n",
    "    \n",
    "    iou_values = []\n",
    "    for y_mask, p_mask in zip(val_masks_stacked, val_binary_prediction):\n",
    "        iou = jaccard_similarity_score(y_mask.flatten(), p_mask.flatten())\n",
    "        iou_values.append(iou)\n",
    "    iou_values = np.array(iou_values)\n",
    "    \n",
    "    accuracies = [\n",
    "        np.mean(iou_values > iou_threshold)\n",
    "        for iou_threshold in np.linspace(0.5, 0.95, 10)\n",
    "    ]\n",
    "    print('Threshold: %.1f, Metric: %.3f' % (threshold, np.mean(accuracies)))\n",
    "    metric_by_threshold.append((np.mean(accuracies), threshold))\n",
    "    \n",
    "best_metric, best_threshold = max(metric_by_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 101, 101)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = best_threshold\n",
    "binary_prediction = (final_pred > threshold).astype(int)\n",
    "\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "all_masks = []\n",
    "for p_mask in list(binary_prediction):\n",
    "    p_mask = rle_encoding(p_mask)\n",
    "    all_masks.append(' '.join(map(str, p_mask)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame([test_file_list, all_masks]).T\n",
    "submit.columns = ['id', 'rle_mask']\n",
    "submit.to_csv('results/submit_unet_sgd_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
