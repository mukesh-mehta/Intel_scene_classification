{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Augmentation\n",
    "##### Horizontal Flip\n",
    "##### Zoom\n",
    "##### shear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from torch.nn import functional as F\n",
    "from torchvision import models\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data'\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TernausNet.unet_models import UNetResNet34,UNet11_bn_dilated,UNet11_bn,UNet11_bn_v2\n",
    "from lovasz_loss import lovasz_hinge\n",
    "\n",
    "save_dir = \"vgg_dil\"\n",
    "def unet_model(**kwargs):\n",
    "    model = UNet11_bn_dilated(**kwargs)\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model():\n",
    "    model = unet_model()\n",
    "    model.train()\n",
    "    return model.cuda()\n",
    "\n",
    "class Loss:\n",
    "    def __init__(self, dice_weight=1):\n",
    "        self.nll_loss = nn.BCELoss()\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        loss = self.nll_loss(outputs, targets)\n",
    "        if self.dice_weight:\n",
    "            eps = 1e-15\n",
    "            dice_target = (targets == 1).float()\n",
    "            dice_output = outputs\n",
    "            intersection = (dice_output * dice_target).sum()\n",
    "            union = dice_output.sum() + dice_target.sum() + eps\n",
    "\n",
    "            dice_loss = 1 - (2 * intersection / union)\n",
    "\n",
    "        return loss - torch.log(dice_loss)\n",
    "    \n",
    "class RobustFocalLoss2d(nn.Module):\n",
    "    #assume top 10% is outliers\n",
    "    def __init__(self, gamma=2, size_average=True):\n",
    "        super(RobustFocalLoss2d, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "\n",
    "\n",
    "    def forward(self, logit, target, class_weight=None, type='softmax'):\n",
    "        target = target.view(-1, 1).long()\n",
    "\n",
    "\n",
    "        if type=='sigmoid':\n",
    "            if class_weight is None:\n",
    "                class_weight = [1]*2 #[0.5, 0.5]\n",
    "\n",
    "#             prob   = F.sigmoid(logit)\n",
    "            prob=logit\n",
    "            prob   = prob.view(-1, 1)\n",
    "            prob   = torch.cat((1-prob, prob), 1)\n",
    "            select = torch.FloatTensor(len(prob), 2).zero_().cuda()\n",
    "            select.scatter_(1, target, 1.)\n",
    "\n",
    "        elif  type=='softmax':\n",
    "            B,C,H,W = logit.size()\n",
    "            if class_weight is None:\n",
    "                class_weight =[1]*C #[1/C]*C\n",
    "\n",
    "            logit   = logit.permute(0, 2, 3, 1).contiguous().view(-1, C)\n",
    "            prob    = F.softmax(logit,1)\n",
    "            select  = torch.FloatTensor(len(prob), C).zero_().cuda()\n",
    "            select.scatter_(1, target, 1.)\n",
    "\n",
    "        class_weight = torch.FloatTensor(class_weight).cuda().view(-1,1)\n",
    "        class_weight = torch.gather(class_weight, 0, target)\n",
    "\n",
    "        prob  = (prob*select).sum(1).view(-1,1)\n",
    "        prob  = torch.clamp(prob,1e-8,1-1e-8)\n",
    "\n",
    "        focus = torch.pow((1-prob), self.gamma)\n",
    "        #focus = torch.where(focus < 2.0, focus, torch.zeros(prob.size()).cuda())\n",
    "        focus = torch.clamp(focus,0,2)\n",
    "\n",
    "\n",
    "        batch_loss = - class_weight *focus*prob.log()\n",
    "\n",
    "        if self.size_average:\n",
    "            loss = batch_loss.mean()\n",
    "        else:\n",
    "            loss = batch_loss\n",
    "\n",
    "        return loss\n",
    "    \n",
    "def accuracy(prob, truth, threshold=0.5,  is_average=True):\n",
    "    batch_size = prob.size(0)\n",
    "    p = prob.detach().view(batch_size,-1)\n",
    "    t = truth.detach().view(batch_size,-1)\n",
    "\n",
    "    p = p>threshold\n",
    "    t = t>0.5\n",
    "    correct = ( p == t).float()\n",
    "    accuracy = correct.sum(1)/p.size(1)\n",
    "\n",
    "    if is_average:\n",
    "        accuracy = accuracy.sum()/batch_size\n",
    "        return accuracy\n",
    "    else:\n",
    "        return accuracy\n",
    "    \n",
    "import numpy as np # linear algebra\n",
    "\n",
    "def get_iou_vector(A, B):\n",
    "    batch_size = A.shape[0]\n",
    "    metric = []\n",
    "    for batch in range(batch_size):\n",
    "        t, p = A[batch].squeeze(1), B[batch].squeeze(1)\n",
    "        if np.count_nonzero(t) == 0 and np.count_nonzero(p) > 0:\n",
    "            metric.append(0)\n",
    "            continue\n",
    "        if np.count_nonzero(t) == 0 and np.count_nonzero(p) == 0:\n",
    "            metric.append(1)\n",
    "            continue\n",
    "\n",
    "        iou = jaccard(t, p)\n",
    "        thresholds = np.arange(0.5, 1, 0.05)\n",
    "        s = []\n",
    "        for thresh in thresholds:\n",
    "            s.append(iou > thresh)\n",
    "        metric.append(np.mean(s))\n",
    "\n",
    "    return np.mean(metric)\n",
    "\n",
    "\n",
    "def jaccard(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    intersection = (y_pred * y_true).sum(dim=-2).sum(dim=-1)\n",
    "    union = y_true.sum(dim=-2).sum(dim=-1) + y_pred.sum(dim=-2).sum(dim=-1)\n",
    "\n",
    "    return ((intersection + epsilon)/ (union - intersection + epsilon)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def random_crop_resize(img, scale=0.75):\n",
    "    center_x, center_y = img.shape[1] / 2, img.shape[0] / 2\n",
    "    width_scaled, height_scaled = img.shape[1] * scale, img.shape[0] * scale\n",
    "    left_x, right_x = center_x - width_scaled / 2, center_x + width_scaled / 2\n",
    "    top_y, bottom_y = center_y - height_scaled / 2, center_y + height_scaled / 2\n",
    "    img_cropped = img[int(top_y):int(bottom_y), int(left_x):int(right_x)]\n",
    "    return cv2.resize(img_cropped,(101,101),interpolation = cv2.INTER_AREA)\n",
    "\n",
    "def increase_brightness(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    h, s, v = cv2.split(hsv)\n",
    "    value =np.random.choice(range(-8,8), 1)[0].astype('uint8')\n",
    "    lim = 255 - value\n",
    "    v[v > lim] = 255\n",
    "    v[v <= lim] += value\n",
    "\n",
    "    final_hsv = cv2.merge((h, s, v))\n",
    "    img = cv2.cvtColor(final_hsv, cv2.COLOR_HSV2BGR)\n",
    "    return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(path, mask = False, h_flip=False, crop=False, brightness = False,ratio = 0.75):\n",
    "    \"\"\"\n",
    "    Load image from a given path and pad it on the sides, so that eash side is divisible by 32 (newtwork requirement)\n",
    "    \n",
    "    if pad = True:\n",
    "        returns image as numpy.array, tuple with padding in pixels as(x_min_pad, y_min_pad, x_max_pad, y_max_pad)\n",
    "    else:\n",
    "        returns image as numpy.array\n",
    "    \"\"\"\n",
    "    img = cv2.imread(str(path))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    if h_flip:\n",
    "        img = cv2.flip( img, 1 )\n",
    "    if crop:\n",
    "        img = random_crop_resize(img,scale=ratio)\n",
    "    \n",
    "    if brightness:\n",
    "        img = increase_brightness(img)\n",
    "        \n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # Padding in needed for UNet models because they need image size to be divisible by 32 \n",
    "    if height % 32 == 0:\n",
    "        y_min_pad = 0\n",
    "        y_max_pad = 0\n",
    "    else:\n",
    "        y_pad = 32 - height % 32\n",
    "        y_min_pad = int(y_pad / 2)\n",
    "        y_max_pad = y_pad - y_min_pad\n",
    "        \n",
    "    if width % 32 == 0:\n",
    "        x_min_pad = 0\n",
    "        x_max_pad = 0\n",
    "    else:\n",
    "        x_pad = 32 - width % 32\n",
    "        x_min_pad = int(x_pad / 2)\n",
    "        x_max_pad = x_pad - x_min_pad\n",
    "    \n",
    "    img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad,cv2.BORDER_REFLECT_101)\n",
    "    if mask:\n",
    "        # Convert mask to 0 and 1 format\n",
    "        img = img[:, :, 0:1] // 255\n",
    "        return torch.from_numpy(img).float().permute([2, 0, 1])\n",
    "    else:\n",
    "        img = img / 255.0\n",
    "        return torch.from_numpy(img).float().permute([2, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "%matplotlib inline\n",
    "img = cv2.imread('../data/train/masks/0a1742c740.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(gray)\n",
    "plt.title('my picture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('../data/train/images/0ba541766e.png')\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(gray)\n",
    "plt.title('my picture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread('../data/train/masks/575d24d81d.png')\n",
    "img = cv2.flip( img, 1 )\n",
    "height, width, _ = img.shape\n",
    "\n",
    "# Padding in needed for UNet models because they need image size to be divisible by 32 \n",
    "if height % 32 == 0:\n",
    "    y_min_pad = 0\n",
    "    y_max_pad = 0\n",
    "else:\n",
    "    y_pad = 32 - height % 32\n",
    "    y_min_pad = int(y_pad / 2)\n",
    "    y_max_pad = y_pad - y_min_pad\n",
    "\n",
    "if width % 32 == 0:\n",
    "    x_min_pad = 0\n",
    "    x_max_pad = 0\n",
    "else:\n",
    "    x_pad = 32 - width % 32\n",
    "    x_min_pad = int(x_pad / 2)\n",
    "    x_max_pad = x_pad - x_min_pad\n",
    "\n",
    "img = cv2.copyMakeBorder(img, y_min_pad, y_max_pad, x_min_pad, x_max_pad, cv2.BORDER_REPLICATE)#cv2.BORDER_REFLECT_101)\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(gray)\n",
    "plt.title('my picture')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms.functional as Func\n",
    "from torch.utils import data\n",
    "import tqdm\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TGSSaltDataset(data.Dataset):\n",
    "    def __init__(self, root_path, file_list, is_test = False, h_flip=False,crop=False,brightness=False,ratio=0.75):\n",
    "        self.is_test = is_test\n",
    "        self.root_path = root_path\n",
    "        self.file_list = file_list\n",
    "        self.h_flip = h_flip\n",
    "        self.crop=crop\n",
    "        self.crop_ratio = ratio\n",
    "        self.brightness = brightness\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        if index not in range(0, len(self.file_list)):\n",
    "            return self.__getitem__(np.random.randint(0, self.__len__()))\n",
    "        \n",
    "        file_id = self.file_list[index]\n",
    "        \n",
    "        image_folder = os.path.join(self.root_path, \"images\")\n",
    "        image_path = os.path.join(image_folder, file_id + \".png\")\n",
    "        \n",
    "        mask_folder = os.path.join(self.root_path, \"masks\")\n",
    "        mask_path = os.path.join(mask_folder, file_id + \".png\")\n",
    "        \n",
    "        image = load_image(image_path, h_flip=self.h_flip,crop = self.crop,ratio = self.crop_ratio,brightness=self.brightness)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return (image,)\n",
    "        else:\n",
    "            mask = load_image(mask_path, mask = True, h_flip=self.h_flip, crop=self.crop,ratio = self.crop_ratio,brightness=self.brightness)\n",
    "            return image,mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# depths_df = pd.read_csv(os.path.join(directory, 'train.csv'))\n",
    "train_df = pd.read_csv(\"../data/train.csv\", index_col=\"id\", usecols=[0,1])\n",
    "train_df[\"masks\"] = [cv2.imread(\"../data/train/masks/{}.png\".format(idx),0) / 255 for idx in train_df.index]\n",
    "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(101, 2)\n",
    "zero_df = train_df[train_df.coverage==0]\n",
    "min_criteria = train_df[train_df.coverage>0.1]\n",
    "train_df = pd.concat([zero_df,min_criteria])\n",
    "def cov_to_class(val):    \n",
    "    for i in range(0, 11):\n",
    "        if val * 10 <= i :\n",
    "            return i\n",
    "train_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
    "train_path = os.path.join(directory, 'train')\n",
    "file_list = list(train_df.index.values)\n",
    "file_list_train,file_list_val  = train_test_split(file_list,test_size=0.1,stratify=train_df.coverage_class, random_state=1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12324"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import transforms, datasets\n",
    "CUDA_LAUNCH_BLOCKING=1\n",
    "# def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "#     state = {'state_dict': model.state_dict(),\n",
    "#              'optimizer' : optimizer.state_dict()}\n",
    "#     torch.save(state, checkpoint_path)\n",
    "#     print('model saved to %s' % checkpoint_path)\n",
    "def save_checkpoint(state, is_best):\n",
    "    filename='sgd_model/'+save_dir+'/best_loss_ph1.pth.tar'\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation loss did not improve\")\n",
    "\n",
    "def save_checkpoint_acc(state, is_best):\n",
    "    filename='sgd_model/'+save_dir+'/best_acc_ph1.pth.tar'\n",
    "    \"\"\"Save checkpoint if a new best is achieved\"\"\"\n",
    "    if is_best:\n",
    "        print (\"=> Saving a new best\")\n",
    "        torch.save(state, filename)  # save checkpoint\n",
    "    else:\n",
    "        print (\"=> Validation Accuracy did not improve\")\n",
    "        \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)\n",
    "    \n",
    "\n",
    "# file_list_train,file_list_dev = train_test_split(file_train,test_size=0.1,random_state=2)\n",
    "dataset = torch.utils.data.ConcatDataset([\n",
    "     TGSSaltDataset(train_path, file_list_train,h_flip=True),\n",
    "     TGSSaltDataset(train_path, file_list_train),\n",
    "    TGSSaltDataset(train_path, file_list_train,h_flip=True,crop=True),\n",
    "     TGSSaltDataset(train_path, file_list_train,crop=True)\n",
    "#     TGSSaltDataset(train_path, file_list_train,h_flip=True,brightness= True),\n",
    "#      TGSSaltDataset(train_path, file_list_train,brightness= True),\n",
    "#     TGSSaltDataset(train_path, file_list_train,h_flip=True,crop=True,brightness= True),\n",
    "#      TGSSaltDataset(train_path, file_list_train,crop=True,brightness= True)\n",
    "])\n",
    "\n",
    "dataset_val = TGSSaltDataset(train_path, file_list_val)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loaded checkpoint 'sgd_model/vgg_dil/best_acc_ph1.pth.tar' (trained for 1 epochs)\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mukesh/miniconda3/envs/tgs_torch/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "100%|| 1027/1027 [11:38<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train loss: 0.140,  Val loss: 0.045\n",
      "Train acc: 0.929, Val acc: 0.941\n",
      "=> Saving a new best\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 0.027,  Val loss: 0.072\n",
      "Train acc: 0.964, Val acc: 0.939\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:25<00:00,  1.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Train loss: 0.013,  Val loss: 0.063\n",
      "Train acc: 0.983, Val acc: 0.952\n",
      "=> Validation loss did not improve\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:44<00:00,  1.46it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Train loss: 0.013,  Val loss: 0.064\n",
      "Train acc: 0.982, Val acc: 0.940\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [13:48<00:00,  1.24it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Train loss: 0.008,  Val loss: 0.119\n",
      "Train acc: 0.988, Val acc: 0.951\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [13:05<00:00,  1.31it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Train loss: 0.008,  Val loss: 0.119\n",
      "Train acc: 0.989, Val acc: 0.949\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:11<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Train loss: 0.021,  Val loss: 0.068\n",
      "Train acc: 0.973, Val acc: 0.945\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:11<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Train loss: 0.017,  Val loss: 0.059\n",
      "Train acc: 0.977, Val acc: 0.950\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:10<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Train loss: 0.010,  Val loss: 0.063\n",
      "Train acc: 0.986, Val acc: 0.948\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Train loss: 0.007,  Val loss: 0.156\n",
      "Train acc: 0.989, Val acc: 0.949\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Train loss: 0.006,  Val loss: 0.122\n",
      "Train acc: 0.991, Val acc: 0.953\n",
      "=> Validation loss did not improve\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:10<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Train loss: 0.005,  Val loss: 0.132\n",
      "Train acc: 0.992, Val acc: 0.953\n",
      "=> Validation loss did not improve\n",
      "=> Saving a new best\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Train loss: 0.013,  Val loss: 0.093\n",
      "Train acc: 0.983, Val acc: 0.943\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:08<00:00,  1.54it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Train loss: 0.014,  Val loss: 0.114\n",
      "Train acc: 0.981, Val acc: 0.947\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Train loss: 0.008,  Val loss: 0.122\n",
      "Train acc: 0.988, Val acc: 0.949\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Train loss: 0.008,  Val loss: 0.118\n",
      "Train acc: 0.989, Val acc: 0.951\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Train loss: 0.005,  Val loss: 0.135\n",
      "Train acc: 0.992, Val acc: 0.949\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Train loss: 0.005,  Val loss: 0.146\n",
      "Train acc: 0.993, Val acc: 0.953\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:08<00:00,  1.54it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Train loss: 0.019,  Val loss: 0.116\n",
      "Train acc: 0.976, Val acc: 0.947\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Train loss: 0.015,  Val loss: 0.077\n",
      "Train acc: 0.979, Val acc: 0.945\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Train loss: 0.009,  Val loss: 0.080\n",
      "Train acc: 0.988, Val acc: 0.947\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Train loss: 0.008,  Val loss: 0.157\n",
      "Train acc: 0.988, Val acc: 0.948\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Train loss: 0.005,  Val loss: 0.143\n",
      "Train acc: 0.992, Val acc: 0.946\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Train loss: 0.004,  Val loss: 0.175\n",
      "Train acc: 0.993, Val acc: 0.949\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Train loss: 0.009,  Val loss: 0.104\n",
      "Train acc: 0.989, Val acc: 0.951\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Train loss: 0.012,  Val loss: 0.150\n",
      "Train acc: 0.983, Val acc: 0.952\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Train loss: 0.008,  Val loss: 0.150\n",
      "Train acc: 0.989, Val acc: 0.953\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Train loss: 0.010,  Val loss: 0.100\n",
      "Train acc: 0.988, Val acc: 0.948\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:08<00:00,  1.54it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Train loss: 0.005,  Val loss: 0.140\n",
      "Train acc: 0.993, Val acc: 0.946\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Train loss: 0.004,  Val loss: 0.207\n",
      "Train acc: 0.993, Val acc: 0.948\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Train loss: 0.007,  Val loss: 0.132\n",
      "Train acc: 0.990, Val acc: 0.952\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 31, Train loss: 0.012,  Val loss: 0.112\n",
      "Train acc: 0.984, Val acc: 0.947\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:09<00:00,  1.53it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 32, Train loss: 0.006,  Val loss: 0.096\n",
      "Train acc: 0.991, Val acc: 0.923\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [18:04<00:00,  1.06s/it]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 33, Train loss: 0.005,  Val loss: 0.157\n",
      "Train acc: 0.993, Val acc: 0.948\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [29:03<00:00,  1.70s/it]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 34, Train loss: 0.004,  Val loss: 0.171\n",
      "Train acc: 0.994, Val acc: 0.947\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [12:05<00:00,  1.42it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 35, Train loss: 0.004,  Val loss: 0.167\n",
      "Train acc: 0.994, Val acc: 0.952\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:51<00:00,  1.44it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 36, Train loss: 0.014,  Val loss: 0.075\n",
      "Train acc: 0.983, Val acc: 0.941\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [12:00<00:00,  1.42it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 37, Train loss: 0.012,  Val loss: 0.162\n",
      "Train acc: 0.985, Val acc: 0.946\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:31<00:00,  1.49it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 38, Train loss: 0.006,  Val loss: 0.150\n",
      "Train acc: 0.991, Val acc: 0.951\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:27<00:00,  1.49it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 39, Train loss: 0.005,  Val loss: 0.193\n",
      "Train acc: 0.993, Val acc: 0.947\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1027/1027 [11:50<00:00,  1.45it/s]\n",
      "  0%|          | 0/1027 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 40, Train loss: 0.004,  Val loss: 0.188\n",
      "Train acc: 0.994, Val acc: 0.947\n",
      "=> Validation loss did not improve\n",
      "=> Validation Accuracy did not improve\n",
      "learning_rate 2.5e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|    | 571/1027 [06:29<05:10,  1.47it/s]"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "resume_weights = \"sgd_model/\"+save_dir+\"/best_acc_ph1.pth.tar\"\n",
    "checkpoint = torch.load(resume_weights)\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_accuracy = checkpoint['best_accuracy']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))\n",
    "# print(model)\n",
    "#\n",
    "def cyclic_lr(epoch, init_lr=1e-4, num_epochs_per_cycle=6, cycle_epochs_decay=2, lr_decay_factor=0.5):\n",
    "    epoch_in_cycle = epoch % num_epochs_per_cycle\n",
    "    lr = init_lr * (lr_decay_factor ** (epoch_in_cycle // cycle_epochs_decay))\n",
    "    return lr\n",
    "# learning_rate = 1e-3\n",
    "# loss_fn = torch.nn.BCELoss()\n",
    "def criterion(logit, truth ):\n",
    "    loss = RobustFocalLoss2d()(logit, truth, type='sigmoid')\n",
    "    return loss\n",
    "\n",
    "def metric(logit, truth, threshold=0.5 ):\n",
    "#     prob = F.sigmoid(logit)\n",
    "    prob = logit\n",
    "    dice = accuracy(prob, truth, threshold=threshold, is_average=True)\n",
    "    return dice\n",
    "# loss_fn = ()\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# optimizer = torch.optim.SGD(model.parameters(),momentum=0.9,lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.01)\n",
    "\n",
    "\n",
    "epoch=60\n",
    "best_loss=1.\n",
    "best_acc = 0.\n",
    "for e in range(epoch):\n",
    "    learning_rate = cyclic_lr(e)\n",
    "    print(\"learning_rate\",learning_rate)\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "#     scheduler.step()\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    for image, mask in tqdm.tqdm(data.DataLoader(dataset, batch_size = 12,shuffle=True, num_workers=6)):\n",
    "        image = image.type(torch.FloatTensor).cuda()\n",
    "        y_pred = model(Variable(image))\n",
    "#         loss = loss_fn(y_pred, mask.to(device))\n",
    "#         loss = lovasz_hinge(y_pred, Variable(mask.cuda()))\n",
    "        loss = criterion(y_pred, Variable(mask.cuda()))\n",
    "        dice  = metric(y_pred, Variable(mask.cuda()))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        train_acc.append(dice.item())\n",
    "        \n",
    "    val_loss = []\n",
    "    val_acc =[]\n",
    "    for image, mask in data.DataLoader(dataset_val, batch_size = 8, shuffle = False):\n",
    "        image = image.cuda()\n",
    "        y_pred = model(Variable(image))\n",
    "\n",
    "#         loss = lovasz_hinge(y_pred, Variable(mask.cuda()))\n",
    "        loss = criterion(y_pred, Variable(mask.cuda()))\n",
    "        dice  = metric(y_pred, Variable(mask.cuda()))\n",
    "        val_loss.append(loss.item())\n",
    "        val_acc.append(dice.item())\n",
    "    print(\"Epoch: %d, Train loss: %.3f,  Val loss: %.3f\" % (e, np.mean(train_loss), np.mean(val_loss)))\n",
    "    print(\"Train acc: %.3f, Val acc: %.3f\" %(np.mean(train_acc),np.mean(val_acc)))\n",
    "    val_loss = np.mean(val_loss)\n",
    "    is_best = bool(val_loss<best_loss)\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_loss = min(val_loss,best_loss)\n",
    "    # Save checkpoint if is a new best\n",
    "    save_checkpoint({\n",
    "        'epoch': e + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_accuracy': best_loss\n",
    "    }, is_best)\n",
    "    val_acc = np.mean(val_acc)\n",
    "    is_best_acc = bool(val_acc>best_acc)\n",
    "    # Get greater Tensor to keep track best acc\n",
    "    best_acc = max(val_acc,best_acc)\n",
    "    # Save checkpoint if is a new best\n",
    "    save_checkpoint_acc({\n",
    "        'epoch': e + 1,\n",
    "        'state_dict': model.state_dict(),\n",
    "        'best_accuracy': best_acc\n",
    "    }, is_best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc,best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9549810221028883, 0.03694632477235309)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc,best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_weights = \"sgd_model/best_acc_ph1.pth.tar\"\n",
    "checkpoint = torch.load(resume_weights)\n",
    "start_epoch = checkpoint['epoch']\n",
    "best_accuracy = checkpoint['best_accuracy']\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"=> loaded checkpoint '{}' (trained for {} epochs)\".format(resume_weights, checkpoint['epoch']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "test_path = os.path.join(directory, 'test')\n",
    "test_file_list = glob.glob(os.path.join(test_path, 'images', '*.png'))\n",
    "test_file_list = [f.split('/')[-1].split('.')[0] for f in test_file_list]\n",
    "test_file_list[:3], test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/2250 [00:00<03:18, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2250/2250 [02:43<00:00, 13.75it/s]\n"
     ]
    }
   ],
   "source": [
    "#NOrmal Image\n",
    "print(len(test_file_list))\n",
    "test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n",
    "\n",
    "all_predictions = []\n",
    "for image in tqdm.tqdm(data.DataLoader(test_dataset, batch_size = 8)):\n",
    "    image = image[0].type(torch.FloatTensor).cuda()\n",
    "    y_pred = model(Variable(image)).cpu().data.numpy()\n",
    "    all_predictions.append(y_pred)\n",
    "all_predictions_stacked = np.vstack(all_predictions)[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2250 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2250/2250 [02:48<00:00, 13.39it/s]\n"
     ]
    }
   ],
   "source": [
    "#Flipped Image\n",
    "print(len(test_file_list))\n",
    "test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True,h_flip=True)\n",
    "\n",
    "all_predictions_flip = []\n",
    "for image in tqdm.tqdm(data.DataLoader(test_dataset, batch_size = 8)):\n",
    "    image = image[0].type(torch.FloatTensor).cuda()\n",
    "    y_pred = model(Variable(image)).cpu().data.numpy()\n",
    "    all_predictions_flip.append(y_pred)\n",
    "all_predictions_stacked_flip = np.vstack(all_predictions_flip)[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Brightness flip Image\n",
    "# print(len(test_file_list))\n",
    "# test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True,h_flip=True,brightness= True)\n",
    "\n",
    "# all_predictions_flip_bright = []\n",
    "# for image in tqdm.tqdm(data.DataLoader(test_dataset, batch_size = 8)):\n",
    "#     image = image[0].type(torch.FloatTensor).cuda()\n",
    "#     y_pred = model(Variable(image)).cpu().data.numpy()\n",
    "#     all_predictions_flip_bright.append(y_pred)\n",
    "# all_predictions_flip_bright = np.vstack(all_predictions_flip_bright)[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Brightness Image\n",
    "# print(len(test_file_list))\n",
    "# test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True,brightness= True)\n",
    "\n",
    "# all_predictions_bright = []\n",
    "# for image in tqdm.tqdm(data.DataLoader(test_dataset, batch_size = 8)):\n",
    "#     image = image[0].type(torch.FloatTensor).cuda()\n",
    "#     y_pred = model(Variable(image)).cpu().data.numpy()\n",
    "#     all_predictions_bright.append(y_pred)\n",
    "# all_predictions_bright = np.vstack(all_predictions_bright)[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 128, 128) (18000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "print(all_predictions_stacked.shape,all_predictions_stacked_flip.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18000, 128, 128)\n"
     ]
    }
   ],
   "source": [
    "final_pred=[]\n",
    "# for pred1, pred2, pred3, pred4 in zip(all_predictions_stacked,all_predictions_stacked_flip,all_predictions_flip_bright,all_predictions_bright):\n",
    "#     final_pred.append((pred1+np.fliplr(pred2)+np.fliplr(pred3)+pred4)/4)\n",
    "for pred1, pred2 in zip(all_predictions_stacked,all_predictions_stacked_flip):\n",
    "    final_pred.append((pred1+np.fliplr(pred2))/2)\n",
    "final_pred = np.array(final_pred)\n",
    "print(final_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width = 101, 101\n",
    "\n",
    "if height % 32 == 0:\n",
    "    y_min_pad = 0\n",
    "    y_max_pad = 0\n",
    "else:\n",
    "    y_pad = 32 - height % 32\n",
    "    y_min_pad = int(y_pad / 2)\n",
    "    y_max_pad = y_pad - y_min_pad\n",
    "\n",
    "if width % 32 == 0:\n",
    "    x_min_pad = 0\n",
    "    x_max_pad = 0\n",
    "else:\n",
    "    x_pad = 32 - width % 32\n",
    "    x_min_pad = int(x_pad / 2)\n",
    "    x_max_pad = x_pad - x_min_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 101, 101)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred = final_pred[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n",
    "final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 86/86 [00:03<00:00, 24.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((343, 101, 101), (343, 101, 101))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = TGSSaltDataset(test_path, test_file_list, is_test = True)\n",
    "\n",
    "val_predictions = []\n",
    "val_masks = []\n",
    "for image, mask in tqdm.tqdm(data.DataLoader(dataset_val, batch_size = 4)):\n",
    "    image = Variable(image.type(torch.FloatTensor).cuda())\n",
    "    y_pred = model(image).cpu().data.numpy()\n",
    "    val_predictions.append(y_pred)\n",
    "    val_masks.append(mask)\n",
    "    \n",
    "val_predictions_stacked = np.vstack(val_predictions)[:, 0, :, :]\n",
    "\n",
    "val_masks_stacked = np.vstack(val_masks)[:, 0, :, :]\n",
    "val_predictions_stacked = val_predictions_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n",
    "\n",
    "val_masks_stacked = val_masks_stacked[:, y_min_pad:128 - y_max_pad, x_min_pad:128 - x_max_pad]\n",
    "val_masks_stacked.shape, val_predictions_stacked.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.0, Metric: 0.157\n",
      "Threshold: 0.1, Metric: 0.472\n",
      "Threshold: 0.2, Metric: 0.711\n",
      "Threshold: 0.3, Metric: 0.857\n",
      "Threshold: 0.4, Metric: 0.901\n",
      "Threshold: 0.5, Metric: 0.915\n",
      "Threshold: 0.6, Metric: 0.905\n",
      "Threshold: 0.7, Metric: 0.881\n",
      "Threshold: 0.8, Metric: 0.844\n",
      "Threshold: 0.9, Metric: 0.783\n",
      "Threshold: 1.0, Metric: 0.583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import jaccard_similarity_score\n",
    "\n",
    "metric_by_threshold = []\n",
    "for threshold in np.linspace(0, 1, 11):\n",
    "    val_binary_prediction = (val_predictions_stacked > threshold).astype(int)\n",
    "    \n",
    "    iou_values = []\n",
    "    for y_mask, p_mask in zip(val_masks_stacked, val_binary_prediction):\n",
    "        iou = jaccard_similarity_score(y_mask.flatten(), p_mask.flatten())\n",
    "        iou_values.append(iou)\n",
    "    iou_values = np.array(iou_values)\n",
    "    \n",
    "    accuracies = [\n",
    "        np.mean(iou_values > iou_threshold)\n",
    "        for iou_threshold in np.linspace(0.5, 0.95, 10)\n",
    "    ]\n",
    "    print('Threshold: %.1f, Metric: %.3f' % (threshold, np.mean(accuracies)))\n",
    "    metric_by_threshold.append((np.mean(accuracies), threshold))\n",
    "    \n",
    "best_metric, best_threshold = max(metric_by_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18000, 101, 101)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = best_threshold\n",
    "binary_prediction = (final_pred > threshold).astype(int)\n",
    "\n",
    "def rle_encoding(x):\n",
    "    dots = np.where(x.T.flatten() == 1)[0]\n",
    "    run_lengths = []\n",
    "    prev = -2\n",
    "    for b in dots:\n",
    "        if (b > prev+1): run_lengths.extend((b + 1, 0))\n",
    "        run_lengths[-1] += 1\n",
    "        prev = b\n",
    "    return run_lengths\n",
    "\n",
    "all_masks = []\n",
    "for p_mask in list(binary_prediction):\n",
    "    p_mask = rle_encoding(p_mask)\n",
    "    all_masks.append(' '.join(map(str, p_mask)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame([test_file_list, all_masks]).T\n",
    "submit.columns = ['id', 'rle_mask']\n",
    "submit.to_csv('results/submit_unet_img_aug_4.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
